#!/usr/bin/env python3
"""
  Execute some (or all) simulation tests, creating ICs if necessary.
  After completion, verify results (vs. analytical or previous numerical solutions), optional visualization.
"""
import argparse
import ast
import enum
import importlib
import os
import os.path
import shutil
import subprocess
import sys
import traceback
from pathlib import Path
import numpy as np

import utils
sys.dont_write_bytecode = True  # do not make .pyc files
os.environ['PYTHONDONTWRITEBYTECODE'] = '1'  # do not make __pycache__ folders

SCRIPT_PATH = Path(__file__).resolve()
PYTHON = 'python3'
CREATE_FILE_NAME = 'create.py'
CHECK_FILE_NAME = 'check.py'
CREATE_FUNCTION_NAME = 'create_ics'
IC_FILE_NAME = 'ics.hdf5'
CONFIG_FILE_NAME = 'Config.sh'
PARAM_FILE_NAME = 'param.txt'

# tests which match BLACKLIST are not included when running all tests
BLACKLIST = [
    # TODO: fix failing AMR tests
    'AMR/',
    # TODO: fix TreePMWithHighResRegion
    'ForceLawTests/TreePMWithHighResRegion',
    # cosmo_zoom_gravity_only_3d requires MUSIC
    'cosmo_zoom_gravity_only_3d',
]

parser = argparse.ArgumentParser(description="""
  Execute some (or all) simulation tests, creating ICs if necessary.
  After completion, verify results (vs. analytical or previous numerical
  solutions), optional visualization.
  """)
parser.add_argument('--withvis',
                    action='store_true',
                    help='create visualization for the test(s) as well')
parser.add_argument(
    '--number-of-tasks',
    type=int,
    default=10,
    help='the number of MPI tasks to use for tests by default (default: 10)')
parser.add_argument(
    '--number-of-compilers',
    type=int,
    default=8,
    help=
    'the number of processes to use for compilation via make -j (default: 8)')
parser.add_argument('--no-cleanup',
                    dest='cleanup',
                    action='store_false',
                    help='do not remove run files after successful tests')
parser.add_argument(
    '--print-timestep',
    type=float,
    default=0.1,
    help='the time step interval for printing occasional test progress '
    '(default: 0.1)')
parser.add_argument('--no-print-output',
                    dest='print_output',
                    action='store_false',
                    help='do not print any text output generated by the tests')
parser.add_argument(
    '--print-all-output',
    action='store_true',
    help='print all text output generated by the test simulations')
parser.add_argument(
    'test_name',
    nargs='+',
    help='The name of a test to run (relative to the examples/ directory). '
    'To run all tests in a row, specify “all”')
args = parser.parse_args()


class TestResult(enum.Enum):
    FAIL = -1
    SKIP = 0
    SUCCESS = 1


def path_in_blacklist(path):
    path_str = str(path)
    for entry in BLACKLIST:
        if entry in path_str:
            return True
    return False


def run_test(name, numTasksMPI=args.number_of_tasks, vis=False):
    """ Run a single test. Return is -1 for failure, 0 for skipped, 1 for success. """
    name = Path(name)
    test_path = SCRIPT_PATH.parent / name
    if (not (test_path / CREATE_FILE_NAME).is_file()
            or not (test_path / CHECK_FILE_NAME).is_file()):
        print('[%s] test not found, skipping.' % name)
        return TestResult.SKIP

    print('[%s]' % name)

    numTasksMPI_default = numTasksMPI

    arepo_root = SCRIPT_PATH.parent.parent
    run_path_abs = arepo_root / 'run' / 'examples' / name
    # relative to the current working directory
    rel_path = Path(os.path.relpath(run_path_abs))
    # relative to AREPO root
    run_path = run_path_abs.relative_to(arepo_root)

    # prepare directory for run
    if rel_path.is_dir():
        print(' - Warning: Run path [%s] already exists, skipping.' % rel_path)
        return 0

    shutil.copytree(test_path, rel_path)
    (rel_path / 'output').mkdir()
    (rel_path / 'build').mkdir()

    # does create_ics() function exist in {test}/create.py?
    has_func = False
    ast_module = None
    try:
        ast_module = ast.parse((test_path / CREATE_FILE_NAME).read_text())
    except SyntaxError:
        pass
    if ast_module is not None:
        for node in ast_module.body:
            if isinstance(
                    node,
                    ast.FunctionDef) and node.name == CREATE_FUNCTION_NAME:
                has_func = True
                break

    print(' - Generating initial conditions...')

    if has_func:
        # new test format: execute create_ics() in {test}/create.py
        pkg_str = '.'.join(name.parts)
        try:
            create = importlib.import_module(pkg_str + '.' +
                                             CREATE_FILE_NAME.rstrip('.py'))
            check = importlib.import_module(pkg_str + '.' +
                                            CHECK_FILE_NAME.rstrip('.py'))
        except:
            utils.fail('FAILED: Error on importing test modules.')
            utils.fail(' ' + traceback.format_exc())
            return TestResult.FAIL

        try:
            getattr(create, CREATE_FUNCTION_NAME)(path=str(rel_path),
                                                  filename=IC_FILE_NAME)
        except:
            utils.fail('FAILED: Could not generate ICs.')
            utils.fail(' ' + traceback.format_exc())
            return TestResult.FAIL

        # allow test to (optionally) specify the number of MPI tasks
        numTasksMPI = getattr(create, 'numTasksMPI', numTasksMPI_default)
        # allow test to (optionally) specify visualization options
        vis_Lx = getattr(create, 'Lx', None)
        vis_Ly = getattr(create, 'Ly', None)

        # vis? update Config.sh and parameter files (except for AMR runs)
        if vis and 'AMR/' not in str(
                name) and vis_Lx is not None and vis_Ly is not None:
            with open(rel_path / CONFIG_FILE_NAME, 'a') as f:
                f.write('VORONOI_FREQUENT_IMAGES\n')
            with open(rel_path / PARAM_FILE_NAME, 'a') as f:
                paramLines = utils.parameterFileVisOptions
                paramLines = paramLines.replace('MAX_X', '%.1f' % vis_Lx)
                paramLines = paramLines.replace('MAX_Y', '%.1f' % vis_Ly)
                f.write(paramLines)
    else:
        # old test format: execute Python file directly
        try:
            cmd = [PYTHON, test_path / CREATE_FILE_NAME, rel_path]
            subprocess.check_output(cmd, stderr=subprocess.STDOUT)
        except subprocess.CalledProcessError as e:
            utils.fail(
                'FAILED: Could not generate ICs (direct call), exit code [%d].'
                % e.returncode)
            if args.print_output:
                utils.fail(' Output:')
                print(str(e.output, 'utf-8'))
            return TestResult.FAIL

    # compile AREPO
    cmd = [
        'make',
        'CONFIG=%s' % (run_path / CONFIG_FILE_NAME),
        'BUILD_DIR=%s' % (run_path / 'build'),
        'EXEC=%s' % (run_path / 'Arepo'), '-j',
        str(args.number_of_compilers)
    ]

    try:
        print(' - Compiling executable...')
        subprocess.check_output(cmd, stderr=subprocess.STDOUT, cwd=arepo_root)
    except subprocess.CalledProcessError as e:
        utils.fail('FAILED: Compilation failed with code [%d].' % e.returncode)
        if args.print_output and e.output:
            utils.fail(' Output:')
            print(str(e.output, 'utf-8'))
        return TestResult.FAIL

    if not (rel_path / 'Arepo').is_file():
        utils.fail(
            'FAILED: Compilation failed to produce an Arepo executable.')
        return TestResult.FAIL

    # execute
    print(' - Compilation done, starting run...')

    cmd = ['mpiexec', '-n', str(numTasksMPI), './Arepo', PARAM_FILE_NAME]

    prev_time = 0.0
    output = ''

    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, cwd=rel_path)

    while True:
        # poll as simulation runs, print occasional progress
        line = p.stdout.readline().decode('utf-8')
        if not line: break
        if prev_time < 0.01: output += line  # store early output

        if args.print_output:
            if args.print_all_output:
                print('     ' + line.strip())
            else:
                if 'Time:' in line:
                    cur_time = float(line.split('Time:')[1].split(',')[0])
                    if cur_time - prev_time >= args.print_timestep:
                        print('     ' + line.strip())
                        prev_time = cur_time
                        continue
                if 'TEST-FORCE-LAW' in line:
                    print('     ' + line.strip())
                    continue

    if 'TERMINATE:' in output:
        utils.fail('FAILED: Run failed with early termination.')
        if args.print_output:
            utils.fail(' Output:')
            print(output)
        return TestResult.FAIL

    try:
        p.wait(timeout=30)
        # check for run failure
        if p.returncode != 0:
            utils.fail('FAILED: Run failed with exit code [%d].' %
                       p.returncode)
            if args.print_output:
                utils.fail(' Output:')
                print(output)
            return TestResult.FAIL
    except subprocess.TimeoutExpired:
        utils.fail('FAILED: Run process did not terminate')
        if args.print_output:
            utils.fail(' Output:')
            print(output)
        return TestResult.FAIL

    # check for failed startup due to parameter file
    if output.count(
            '\n') < 10000 and 'Error in file %s:' % PARAM_FILE_NAME in output:
        utils.fail('FAILED:')
        for line in output.split('\n'):
            if 'Error' in line: utils.fail(line)
        return TestResult.FAIL

    print(' - Run done, verifying results...')

    if has_func:
        # new test format: execute verify_result() function in {test}/check.py
        try:
            status_ok, info = check.verify_result(str(rel_path))
            if not status_ok:
                utils.fail('FAILED.')
                for msg in info:
                    utils.fail(' ' + msg)
                return TestResult.FAIL
        except:
            utils.fail('FAILED.')
            utils.fail(' ' + traceback.format_exc())
            return TestResult.FAIL
        else:
            utils.success('SUCCESS.')
            for msg in info:
                utils.success(' ' + msg)
        # visualization requested? call visualize_result() of the test, leave output
        if vis:
            print(' - Creating visualization(s)...')
            if not (rel_path / 'vis').is_dir():
                (rel_path / 'vis').mkdir()
            try:
                check.visualize_result(str(rel_path), vis_Lx, vis_Ly)
            except :
                print(' - An error occured during creation of visualization:')
                traceback.print_exc()
    else:
        # old test format: execute Python file directly
        try:
            cmd = [PYTHON, test_path / CHECK_FILE_NAME, rel_path, str(vis)]
            subprocess.check_output(cmd, stderr=subprocess.STDOUT)
        except subprocess.CalledProcessError as e:
            utils.fail(
                'FAILED: Could not verify results or verification failed (direct call), exit code [%d].'
                % e.returncode)
            if args.print_output and e.output:
                utils.fail(' Output:')
                print(str(e.output, 'utf-8'))
            return TestResult.FAIL
        utils.success('SUCCESS.')

    if args.cleanup and not vis:
        # cleanup
        shutil.rmtree(rel_path)

    return TestResult.SUCCESS


def main():
    # which run, or all?
    examples_dir = SCRIPT_PATH.parent
    testNames = []
    if 'all' in args.test_name:
        testNames += [
            path.parent.relative_to(examples_dir)
            for path in sorted(examples_dir.glob('**/' + CREATE_FILE_NAME),
                               key=lambda p: str(p).casefold())
            if not path_in_blacklist(path)
        ]
    testNames += [Path(name) for name in args.test_name if name != 'all']
    # run tests sequentially, store success/failure status of each
    results = []
    for testName in testNames:
        cwd = Path.cwd()
        sys_path = sys.path.copy()
        results.append(run_test(testName, vis=args.withvis))
        # restore potentially modified CWD and sys.path
        if cwd != Path.cwd():
            os.chdir(cwd)
        sys.path = sys_path
        print()
    assert len(testNames) == len(results)
    # print summary if we ran more than one test
    if len(testNames) > 1:
        status = utils.success if results.count(
            TestResult.FAIL) == 0 else utils.fail
        status(
            'All tests done. [%d] skipped, [%d] succeeded, [%d] failed.' %
            (results.count(TestResult.SKIP), results.count(
                TestResult.SUCCESS), results.count(TestResult.FAIL)))
        if status == utils.fail:
            utils.fail('Failed tests:')
        for test_idx in range(len(testNames)):
            if results[test_idx] == TestResult.FAIL:
                utils.fail('\t[%s]' % testNames[test_idx])


if __name__ == '__main__':
    main()
